2022-08-30T06:50:49,008 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-08-30T06:50:49,008 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-08-30T06:50:49,057 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-08-30T06:50:49,057 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-08-30T06:50:49,202 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/venv/lib/python3.8/site-packages
Current directory: /home/model-server
Temp directory: /home/model-server/tmp
Number of GPUs: 1
Number of CPUs: 16
Max heap size: 7992 M
Python executable: /home/venv/bin/python
Config file: logs/config/20220829145919748-startup.cfg
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /tmp/models
Initial Models: text_classifier=text_classifier.mar
Log dir: /home/model-server/logs
Metrics dir: /home/model-server/logs
Netty threads: 32
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /tmp/models
Model config: N/A
2022-08-30T06:50:49,202 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/venv/lib/python3.8/site-packages
Current directory: /home/model-server
Temp directory: /home/model-server/tmp
Number of GPUs: 1
Number of CPUs: 16
Max heap size: 7992 M
Python executable: /home/venv/bin/python
Config file: logs/config/20220829145919748-startup.cfg
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /tmp/models
Initial Models: text_classifier=text_classifier.mar
Log dir: /home/model-server/logs
Metrics dir: /home/model-server/logs
Netty threads: 32
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /tmp/models
Model config: N/A
2022-08-30T06:50:49,212 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220829145919748-startup.cfg",
  "modelCount": 0,
  "created": 1661785159748,
  "models": {}
}
2022-08-30T06:50:49,212 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220829145919748-startup.cfg",
  "modelCount": 0,
  "created": 1661785159748,
  "models": {}
}
2022-08-30T06:50:49,217 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220829145919748-startup.cfg
2022-08-30T06:50:49,217 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220829145919748-startup.cfg
2022-08-30T06:50:49,218 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220829145919748-startup.cfg validated successfully
2022-08-30T06:50:49,218 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220829145919748-startup.cfg validated successfully
2022-08-30T06:50:49,218 [WARN ] main org.pytorch.serve.snapshot.SnapshotManager - Model snapshot is empty. Starting TorchServe without initial models.
2022-08-30T06:50:49,218 [WARN ] main org.pytorch.serve.snapshot.SnapshotManager - Model snapshot is empty. Starting TorchServe without initial models.
2022-08-30T06:50:49,220 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-08-30T06:50:49,220 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-08-30T06:50:49,270 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-08-30T06:50:49,270 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-08-30T06:50:49,271 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-08-30T06:50:49,271 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-08-30T06:50:49,271 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-08-30T06:50:49,271 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-08-30T06:50:49,272 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-08-30T06:50:49,272 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-08-30T06:50:49,272 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-08-30T06:50:49,272 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-08-30T06:50:50,194 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:86d5c1074db1,timestamp:1661842250
2022-08-30T06:50:50,197 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:51.5458869934082|#Level:Host|#hostname:86d5c1074db1,timestamp:1661842250
2022-08-30T06:50:50,197 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:392.5061454772949|#Level:Host|#hostname:86d5c1074db1,timestamp:1661842250
2022-08-30T06:50:50,197 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:88.4|#Level:Host|#hostname:86d5c1074db1,timestamp:1661842250
2022-08-30T06:50:50,198 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:8.235677083333334|#Level:Host,device_id:0|#hostname:86d5c1074db1,timestamp:1661842250
2022-08-30T06:50:50,198 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:506|#Level:Host,device_id:0|#hostname:86d5c1074db1,timestamp:1661842250
2022-08-30T06:50:50,198 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:1|#Level:Host,device_id:0|#hostname:86d5c1074db1,timestamp:1661842250
2022-08-30T06:50:50,198 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:26295.54296875|#Level:Host|#hostname:86d5c1074db1,timestamp:1661842250
2022-08-30T06:50:50,199 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:5089.3671875|#Level:Host|#hostname:86d5c1074db1,timestamp:1661842250
2022-08-30T06:50:50,199 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:17.7|#Level:Host|#hostname:86d5c1074db1,timestamp:1661842250
2022-08-30T06:50:57,832 [INFO ] epollEventLoopGroup-3-1 ACCESS_LOG - /172.30.0.1:59692 "GET /models HTTP/1.1" 200 3
2022-08-30T06:50:57,833 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:86d5c1074db1,timestamp:1661842257
2022-08-30T06:55:34,585 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-08-30T06:55:34,585 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-08-30T06:55:34,714 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/venv/lib/python3.8/site-packages
Current directory: /home/model-server
Temp directory: /home/model-server/tmp
Number of GPUs: 1
Number of CPUs: 16
Max heap size: 7992 M
Python executable: /home/venv/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /tmp/model_store
Initial Models: densenet161.mar
Log dir: /home/model-server/logs
Metrics dir: /home/model-server/logs
Netty threads: 32
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /tmp/model_store
Model config: N/A
2022-08-30T06:55:34,714 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/venv/lib/python3.8/site-packages
Current directory: /home/model-server
Temp directory: /home/model-server/tmp
Number of GPUs: 1
Number of CPUs: 16
Max heap size: 7992 M
Python executable: /home/venv/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /tmp/model_store
Initial Models: densenet161.mar
Log dir: /home/model-server/logs
Metrics dir: /home/model-server/logs
Netty threads: 32
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /tmp/model_store
Model config: N/A
2022-08-30T06:55:34,722 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-08-30T06:55:34,722 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-08-30T06:55:34,740 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: densenet161.mar
2022-08-30T06:55:34,740 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: densenet161.mar
2022-08-30T06:55:35,719 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model densenet161
2022-08-30T06:55:35,719 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model densenet161
2022-08-30T06:55:35,719 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model densenet161
2022-08-30T06:55:35,719 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model densenet161
2022-08-30T06:55:35,719 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model densenet161 loaded.
2022-08-30T06:55:35,719 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model densenet161 loaded.
2022-08-30T06:55:35,719 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: densenet161, count: 1
2022-08-30T06:55:35,719 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: densenet161, count: 1
2022-08-30T06:55:35,727 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000]
2022-08-30T06:55:35,727 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000]
2022-08-30T06:55:35,730 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-08-30T06:55:35,730 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-08-30T06:55:35,779 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-08-30T06:55:35,779 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-08-30T06:55:35,779 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-08-30T06:55:35,779 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-08-30T06:55:35,780 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-08-30T06:55:35,780 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-08-30T06:55:35,781 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-08-30T06:55:35,781 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-08-30T06:55:35,782 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-08-30T06:55:35,782 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-08-30T06:55:35,974 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-08-30T06:55:35,974 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-08-30T06:55:36,465 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:7f3d8428b172,timestamp:1661842536
2022-08-30T06:55:36,466 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:50.11610412597656|#Level:Host|#hostname:7f3d8428b172,timestamp:1661842536
2022-08-30T06:55:36,466 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:393.93592834472656|#Level:Host|#hostname:7f3d8428b172,timestamp:1661842536
2022-08-30T06:55:36,466 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:88.7|#Level:Host|#hostname:7f3d8428b172,timestamp:1661842536
2022-08-30T06:55:36,466 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:11.42578125|#Level:Host,device_id:0|#hostname:7f3d8428b172,timestamp:1661842536
2022-08-30T06:55:36,467 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:702|#Level:Host,device_id:0|#hostname:7f3d8428b172,timestamp:1661842536
2022-08-30T06:55:36,467 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:7f3d8428b172,timestamp:1661842536
2022-08-30T06:55:36,467 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:24651.52734375|#Level:Host|#hostname:7f3d8428b172,timestamp:1661842536
2022-08-30T06:55:36,467 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6699.6328125|#Level:Host|#hostname:7f3d8428b172,timestamp:1661842536
2022-08-30T06:55:36,468 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:22.9|#Level:Host|#hostname:7f3d8428b172,timestamp:1661842536
2022-08-30T06:55:36,641 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:55:36,645 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - [PID]54
2022-08-30T06:55:36,645 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Torch worker started.
2022-08-30T06:55:36,645 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Python runtime: 3.8.0
2022-08-30T06:55:36,646 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change null -> WORKER_STARTED
2022-08-30T06:55:36,646 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change null -> WORKER_STARTED
2022-08-30T06:55:36,652 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:55:36,652 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:55:36,660 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.
2022-08-30T06:55:36,662 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661842536662
2022-08-30T06:55:36,662 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661842536662
2022-08-30T06:55:36,685 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - model_name: densenet161, batchSize: 1
2022-08-30T06:55:37,283 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - generated new fontManager
2022-08-30T06:55:39,485 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2800
2022-08-30T06:55:39,485 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2800
2022-08-30T06:55:39,486 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-08-30T06:55:39,486 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-08-30T06:55:39,486 [INFO ] W-9000-densenet161_1.0 TS_METRICS - W-9000-densenet161_1.0.ms:3762|#Level:Host|#hostname:7f3d8428b172,timestamp:1661842539
2022-08-30T06:55:39,487 [INFO ] W-9000-densenet161_1.0 TS_METRICS - WorkerThreadTime.ms:25|#Level:Host|#hostname:7f3d8428b172,timestamp:1661842539
2022-08-30T06:55:49,217 [INFO ] epollEventLoopGroup-3-1 ACCESS_LOG - /172.30.0.1:59694 "GET /models HTTP/1.1" 200 4
2022-08-30T06:55:49,218 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:7f3d8428b172,timestamp:1661842549
2022-08-30T06:56:42,415 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-08-30T06:56:42,415 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-08-30T06:56:42,575 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/venv/lib/python3.8/site-packages
Current directory: /home/model-server
Temp directory: /home/model-server/tmp
Number of GPUs: 1
Number of CPUs: 16
Max heap size: 7992 M
Python executable: /home/venv/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /tmp/models
Initial Models: densenet161.mar
Log dir: /home/model-server/logs
Metrics dir: /home/model-server/logs
Netty threads: 32
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /tmp/models
Model config: N/A
2022-08-30T06:56:42,575 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/venv/lib/python3.8/site-packages
Current directory: /home/model-server
Temp directory: /home/model-server/tmp
Number of GPUs: 1
Number of CPUs: 16
Max heap size: 7992 M
Python executable: /home/venv/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /tmp/models
Initial Models: densenet161.mar
Log dir: /home/model-server/logs
Metrics dir: /home/model-server/logs
Netty threads: 32
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /tmp/models
Model config: N/A
2022-08-30T06:56:42,583 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-08-30T06:56:42,583 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-08-30T06:56:42,601 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: densenet161.mar
2022-08-30T06:56:42,601 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: densenet161.mar
2022-08-30T06:56:43,601 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model densenet161
2022-08-30T06:56:43,601 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model densenet161
2022-08-30T06:56:43,601 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model densenet161
2022-08-30T06:56:43,601 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model densenet161
2022-08-30T06:56:43,601 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model densenet161 loaded.
2022-08-30T06:56:43,601 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model densenet161 loaded.
2022-08-30T06:56:43,602 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: densenet161, count: 1
2022-08-30T06:56:43,602 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: densenet161, count: 1
2022-08-30T06:56:43,607 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000]
2022-08-30T06:56:43,607 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000]
2022-08-30T06:56:43,608 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-08-30T06:56:43,608 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-08-30T06:56:43,653 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-08-30T06:56:43,653 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-08-30T06:56:43,653 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-08-30T06:56:43,653 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-08-30T06:56:43,654 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-08-30T06:56:43,654 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-08-30T06:56:43,655 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-08-30T06:56:43,655 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-08-30T06:56:43,655 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-08-30T06:56:43,655 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-08-30T06:56:43,837 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-08-30T06:56:43,837 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-08-30T06:56:44,200 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:56:44,201 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - [PID]54
2022-08-30T06:56:44,201 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Torch worker started.
2022-08-30T06:56:44,201 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Python runtime: 3.8.0
2022-08-30T06:56:44,202 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change null -> WORKER_STARTED
2022-08-30T06:56:44,202 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change null -> WORKER_STARTED
2022-08-30T06:56:44,206 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:56:44,206 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:56:44,213 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.
2022-08-30T06:56:44,216 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661842604216
2022-08-30T06:56:44,216 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661842604216
2022-08-30T06:56:44,238 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - model_name: densenet161, batchSize: 1
2022-08-30T06:56:44,413 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:c1f4ff45b476,timestamp:1661842604
2022-08-30T06:56:44,413 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:49.867835998535156|#Level:Host|#hostname:c1f4ff45b476,timestamp:1661842604
2022-08-30T06:56:44,414 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:394.18419647216797|#Level:Host|#hostname:c1f4ff45b476,timestamp:1661842604
2022-08-30T06:56:44,414 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:88.8|#Level:Host|#hostname:c1f4ff45b476,timestamp:1661842604
2022-08-30T06:56:44,414 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:11.588541666666666|#Level:Host,device_id:0|#hostname:c1f4ff45b476,timestamp:1661842604
2022-08-30T06:56:44,414 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:712|#Level:Host,device_id:0|#hostname:c1f4ff45b476,timestamp:1661842604
2022-08-30T06:56:44,414 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:56|#Level:Host,device_id:0|#hostname:c1f4ff45b476,timestamp:1661842604
2022-08-30T06:56:44,415 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:24551.11328125|#Level:Host|#hostname:c1f4ff45b476,timestamp:1661842604
2022-08-30T06:56:44,415 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6791.80078125|#Level:Host|#hostname:c1f4ff45b476,timestamp:1661842604
2022-08-30T06:56:44,415 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:23.2|#Level:Host|#hostname:c1f4ff45b476,timestamp:1661842604
2022-08-30T06:56:44,704 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - generated new fontManager
2022-08-30T06:56:46,545 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2307
2022-08-30T06:56:46,545 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2307
2022-08-30T06:56:46,546 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-08-30T06:56:46,546 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-08-30T06:56:46,546 [INFO ] W-9000-densenet161_1.0 TS_METRICS - W-9000-densenet161_1.0.ms:2941|#Level:Host|#hostname:c1f4ff45b476,timestamp:1661842606
2022-08-30T06:56:46,546 [INFO ] W-9000-densenet161_1.0 TS_METRICS - WorkerThreadTime.ms:23|#Level:Host|#hostname:c1f4ff45b476,timestamp:1661842606
2022-08-30T06:56:47,755 [INFO ] epollEventLoopGroup-3-1 ACCESS_LOG - /172.30.0.1:59696 "GET /models HTTP/1.1" 200 4
2022-08-30T06:56:47,755 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:c1f4ff45b476,timestamp:1661842607
2022-08-30T06:57:00,516 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-08-30T06:57:00,516 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-08-30T06:57:00,664 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/venv/lib/python3.8/site-packages
Current directory: /home/model-server
Temp directory: /home/model-server/tmp
Number of GPUs: 1
Number of CPUs: 16
Max heap size: 7992 M
Python executable: /home/venv/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /tmp/models
Initial Models: all
Log dir: /home/model-server/logs
Metrics dir: /home/model-server/logs
Netty threads: 32
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /tmp/models
Model config: N/A
2022-08-30T06:57:00,664 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/venv/lib/python3.8/site-packages
Current directory: /home/model-server
Temp directory: /home/model-server/tmp
Number of GPUs: 1
Number of CPUs: 16
Max heap size: 7992 M
Python executable: /home/venv/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /tmp/models
Initial Models: all
Log dir: /home/model-server/logs
Metrics dir: /home/model-server/logs
Netty threads: 32
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /tmp/models
Model config: N/A
2022-08-30T06:57:00,674 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-08-30T06:57:00,674 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-08-30T06:57:00,695 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: densenet161.mar
2022-08-30T06:57:00,695 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: densenet161.mar
2022-08-30T06:57:01,642 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model densenet161
2022-08-30T06:57:01,642 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model densenet161
2022-08-30T06:57:01,643 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model densenet161
2022-08-30T06:57:01,643 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model densenet161
2022-08-30T06:57:01,643 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model densenet161 loaded.
2022-08-30T06:57:01,643 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model densenet161 loaded.
2022-08-30T06:57:01,643 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: densenet161, count: 1
2022-08-30T06:57:01,643 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: densenet161, count: 1
2022-08-30T06:57:01,649 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000]
2022-08-30T06:57:01,649 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000]
2022-08-30T06:57:01,650 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-08-30T06:57:01,650 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-08-30T06:57:01,695 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-08-30T06:57:01,695 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-08-30T06:57:01,695 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-08-30T06:57:01,695 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-08-30T06:57:01,696 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-08-30T06:57:01,696 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-08-30T06:57:01,696 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-08-30T06:57:01,696 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-08-30T06:57:01,697 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-08-30T06:57:01,697 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-08-30T06:57:01,858 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-08-30T06:57:01,858 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-08-30T06:57:02,225 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:57:02,226 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - [PID]54
2022-08-30T06:57:02,226 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Torch worker started.
2022-08-30T06:57:02,227 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Python runtime: 3.8.0
2022-08-30T06:57:02,227 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change null -> WORKER_STARTED
2022-08-30T06:57:02,227 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change null -> WORKER_STARTED
2022-08-30T06:57:02,230 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:57:02,230 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:57:02,238 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.
2022-08-30T06:57:02,241 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661842622241
2022-08-30T06:57:02,241 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661842622241
2022-08-30T06:57:02,264 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - model_name: densenet161, batchSize: 1
2022-08-30T06:57:02,426 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842622
2022-08-30T06:57:02,427 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:49.79938507080078|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842622
2022-08-30T06:57:02,427 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:394.25264739990234|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842622
2022-08-30T06:57:02,427 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:88.8|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842622
2022-08-30T06:57:02,427 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:11.930338541666666|#Level:Host,device_id:0|#hostname:148f9db9e5dd,timestamp:1661842622
2022-08-30T06:57:02,428 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:733|#Level:Host,device_id:0|#hostname:148f9db9e5dd,timestamp:1661842622
2022-08-30T06:57:02,428 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:46|#Level:Host,device_id:0|#hostname:148f9db9e5dd,timestamp:1661842622
2022-08-30T06:57:02,428 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:24517.375|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842622
2022-08-30T06:57:02,428 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6827.0|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842622
2022-08-30T06:57:02,428 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:23.3|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842622
2022-08-30T06:57:02,724 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - generated new fontManager
2022-08-30T06:57:04,644 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2380
2022-08-30T06:57:04,644 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2380
2022-08-30T06:57:04,644 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-08-30T06:57:04,644 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-08-30T06:57:04,645 [INFO ] W-9000-densenet161_1.0 TS_METRICS - W-9000-densenet161_1.0.ms:2997|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842624
2022-08-30T06:57:04,645 [INFO ] W-9000-densenet161_1.0 TS_METRICS - WorkerThreadTime.ms:24|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842624
2022-08-30T06:57:05,403 [INFO ] epollEventLoopGroup-3-1 ACCESS_LOG - /172.30.0.1:59698 "GET /models HTTP/1.1" 200 5
2022-08-30T06:57:05,403 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842625
2022-08-30T06:57:34,619 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-08-30T06:57:34,619 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-08-30T06:57:34,770 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/venv/lib/python3.8/site-packages
Current directory: /home/model-server
Temp directory: /home/model-server/tmp
Number of GPUs: 1
Number of CPUs: 16
Max heap size: 7992 M
Python executable: /home/venv/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /tmp/models
Initial Models: all
Log dir: /home/model-server/logs
Metrics dir: /home/model-server/logs
Netty threads: 32
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /tmp/models
Model config: N/A
2022-08-30T06:57:34,770 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/venv/lib/python3.8/site-packages
Current directory: /home/model-server
Temp directory: /home/model-server/tmp
Number of GPUs: 1
Number of CPUs: 16
Max heap size: 7992 M
Python executable: /home/venv/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /tmp/models
Initial Models: all
Log dir: /home/model-server/logs
Metrics dir: /home/model-server/logs
Netty threads: 32
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /tmp/models
Model config: N/A
2022-08-30T06:57:34,778 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-08-30T06:57:34,778 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-08-30T06:57:34,795 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: image_classifier
2022-08-30T06:57:34,795 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: image_classifier
2022-08-30T06:57:34,798 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-08-30T06:57:34,798 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-08-30T06:57:34,798 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-08-30T06:57:34,798 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-08-30T06:57:34,800 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model image_classifier
2022-08-30T06:57:34,800 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model image_classifier
2022-08-30T06:57:34,800 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model image_classifier
2022-08-30T06:57:34,800 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model image_classifier
2022-08-30T06:57:34,800 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model image_classifier loaded.
2022-08-30T06:57:34,800 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model image_classifier loaded.
2022-08-30T06:57:34,800 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: image_classifier, count: 1
2022-08-30T06:57:34,800 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: image_classifier, count: 1
2022-08-30T06:57:34,806 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000]
2022-08-30T06:57:34,806 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000]
2022-08-30T06:57:34,807 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-08-30T06:57:34,807 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-08-30T06:57:34,851 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-08-30T06:57:34,851 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-08-30T06:57:34,852 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-08-30T06:57:34,852 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-08-30T06:57:34,853 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-08-30T06:57:34,853 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-08-30T06:57:34,853 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-08-30T06:57:34,853 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-08-30T06:57:34,853 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-08-30T06:57:34,853 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-08-30T06:57:35,034 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-08-30T06:57:35,034 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-08-30T06:57:35,376 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:57:35,377 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - [PID]48
2022-08-30T06:57:35,378 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Torch worker started.
2022-08-30T06:57:35,378 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Python runtime: 3.8.0
2022-08-30T06:57:35,378 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change null -> WORKER_STARTED
2022-08-30T06:57:35,378 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change null -> WORKER_STARTED
2022-08-30T06:57:35,382 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:57:35,382 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:57:35,389 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.
2022-08-30T06:57:35,391 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661842655391
2022-08-30T06:57:35,391 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661842655391
2022-08-30T06:57:35,414 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - model_name: image_classifier, batchSize: 1
2022-08-30T06:57:35,415 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Backend worker process died.
2022-08-30T06:57:35,415 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-08-30T06:57:35,416 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_loader.py", line 100, in load
2022-08-30T06:57:35,416 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-08-30T06:57:35,416 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-08-30T06:57:35,416 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-08-30T06:57:35,416 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-08-30T06:57:35,416 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-08-30T06:57:35,417 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-08-30T06:57:35,417 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-08-30T06:57:35,417 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-08-30T06:57:35,417 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-08-30T06:57:35,417 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-08-30T06:57:35,418 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-08-30T06:57:35,418 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-08-30T06:57:35,418 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'image_classifier'
2022-08-30T06:57:35,418 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - 
2022-08-30T06:57:35,420 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-08-30T06:57:35,420 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - 
2022-08-30T06:57:35,421 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-08-30T06:57:35,421 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-08-30T06:57:35,421 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     worker.run_server()
2022-08-30T06:57:35,421 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-08-30T06:57:35,421 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-08-30T06:57:35,422 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-08-30T06:57:35,422 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-08-30T06:57:35,422 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-08-30T06:57:35,422 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-08-30T06:57:35,422 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_loader.py", line 102, in load
2022-08-30T06:57:35,423 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-08-30T06:57:35,423 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_loader.py", line 167, in _load_default_handler
2022-08-30T06:57:35,423 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-08-30T06:57:35,423 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-08-30T06:57:35,423 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-08-30T06:57:35,424 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-08-30T06:57:35,424 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-08-30T06:57:35,425 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-08-30T06:57:35,425 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler./tmp/models/image_classifier'
2022-08-30T06:57:35,418 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-08-30T06:57:35,418 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-08-30T06:57:35,432 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: image_classifier, error: Worker died.
2022-08-30T06:57:35,432 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: image_classifier, error: Worker died.
2022-08-30T06:57:35,433 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-08-30T06:57:35,433 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-08-30T06:57:35,435 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:35,435 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:35,436 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:35,436 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:35,436 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-08-30T06:57:35,436 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-08-30T06:57:35,441 [INFO ] W-9000-image_classifier_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:35,441 [INFO ] W-9000-image_classifier_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:35,441 [INFO ] W-9000-image_classifier_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:35,441 [INFO ] W-9000-image_classifier_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:35,630 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:66.7|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842655
2022-08-30T06:57:35,631 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:49.66150665283203|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842655
2022-08-30T06:57:35,631 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:394.3905258178711|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842655
2022-08-30T06:57:35,631 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:88.8|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842655
2022-08-30T06:57:35,631 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:11.083984375|#Level:Host,device_id:0|#hostname:148f9db9e5dd,timestamp:1661842655
2022-08-30T06:57:35,632 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:681|#Level:Host,device_id:0|#hostname:148f9db9e5dd,timestamp:1661842655
2022-08-30T06:57:35,632 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:43|#Level:Host,device_id:0|#hostname:148f9db9e5dd,timestamp:1661842655
2022-08-30T06:57:35,632 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:24667.08984375|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842655
2022-08-30T06:57:35,632 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6688.1640625|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842655
2022-08-30T06:57:35,633 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:22.8|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842655
2022-08-30T06:57:36,437 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000]
2022-08-30T06:57:36,437 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000]
2022-08-30T06:57:36,976 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:57:36,977 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - [PID]96
2022-08-30T06:57:36,978 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-08-30T06:57:36,978 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Torch worker started.
2022-08-30T06:57:36,978 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-08-30T06:57:36,978 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Python runtime: 3.8.0
2022-08-30T06:57:36,978 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:57:36,978 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:57:36,979 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661842656979
2022-08-30T06:57:36,979 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.
2022-08-30T06:57:36,979 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661842656979
2022-08-30T06:57:36,990 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - model_name: image_classifier, batchSize: 1
2022-08-30T06:57:36,991 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Backend worker process died.
2022-08-30T06:57:36,992 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-08-30T06:57:36,992 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-08-30T06:57:36,992 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_loader.py", line 100, in load
2022-08-30T06:57:36,992 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-08-30T06:57:36,992 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-08-30T06:57:36,992 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-08-30T06:57:36,992 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-08-30T06:57:36,992 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-08-30T06:57:36,993 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-08-30T06:57:36,993 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-08-30T06:57:36,993 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-08-30T06:57:36,993 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-08-30T06:57:36,993 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-08-30T06:57:36,993 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: image_classifier, error: Worker died.
2022-08-30T06:57:36,993 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-08-30T06:57:36,993 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: image_classifier, error: Worker died.
2022-08-30T06:57:36,994 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-08-30T06:57:36,994 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-08-30T06:57:36,994 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-08-30T06:57:36,994 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-08-30T06:57:36,994 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:36,994 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:36,994 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'image_classifier'
2022-08-30T06:57:36,994 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:36,994 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:36,994 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - 
2022-08-30T06:57:36,994 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-08-30T06:57:36,994 [INFO ] W-9000-image_classifier_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:36,994 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-08-30T06:57:36,994 [INFO ] W-9000-image_classifier_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:37,003 [INFO ] W-9000-image_classifier_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:37,003 [INFO ] W-9000-image_classifier_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:37,748 [INFO ] epollEventLoopGroup-3-1 ACCESS_LOG - /172.30.0.1:59700 "GET /models HTTP/1.1" 200 5
2022-08-30T06:57:37,748 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842657
2022-08-30T06:57:37,996 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000]
2022-08-30T06:57:37,996 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000]
2022-08-30T06:57:38,535 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:57:38,536 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - [PID]117
2022-08-30T06:57:38,536 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Torch worker started.
2022-08-30T06:57:38,536 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-08-30T06:57:38,536 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-08-30T06:57:38,536 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Python runtime: 3.8.0
2022-08-30T06:57:38,536 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:57:38,536 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:57:38,537 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661842658537
2022-08-30T06:57:38,537 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.
2022-08-30T06:57:38,537 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661842658537
2022-08-30T06:57:38,548 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - model_name: image_classifier, batchSize: 1
2022-08-30T06:57:38,549 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Backend worker process died.
2022-08-30T06:57:38,549 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-08-30T06:57:38,549 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_loader.py", line 100, in load
2022-08-30T06:57:38,549 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-08-30T06:57:38,550 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-08-30T06:57:38,549 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-08-30T06:57:38,550 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-08-30T06:57:38,550 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-08-30T06:57:38,550 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-08-30T06:57:38,550 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-08-30T06:57:38,550 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-08-30T06:57:38,550 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-08-30T06:57:38,551 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-08-30T06:57:38,551 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-08-30T06:57:38,550 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-08-30T06:57:38,551 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-08-30T06:57:38,550 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-08-30T06:57:38,552 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: image_classifier, error: Worker died.
2022-08-30T06:57:38,552 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: image_classifier, error: Worker died.
2022-08-30T06:57:38,551 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'image_classifier'
2022-08-30T06:57:38,552 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-08-30T06:57:38,552 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-08-30T06:57:38,552 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - 
2022-08-30T06:57:38,552 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-08-30T06:57:38,552 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:38,552 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:38,552 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:38,552 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - 
2022-08-30T06:57:38,552 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:38,552 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-08-30T06:57:38,552 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-08-30T06:57:38,553 [INFO ] W-9000-image_classifier_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:38,552 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-08-30T06:57:38,553 [INFO ] W-9000-image_classifier_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:38,562 [INFO ] W-9000-image_classifier_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:38,562 [INFO ] W-9000-image_classifier_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:40,553 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000]
2022-08-30T06:57:40,553 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000]
2022-08-30T06:57:41,079 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:57:41,079 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - [PID]140
2022-08-30T06:57:41,080 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Torch worker started.
2022-08-30T06:57:41,080 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-08-30T06:57:41,080 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Python runtime: 3.8.0
2022-08-30T06:57:41,080 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-08-30T06:57:41,080 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:57:41,080 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:57:41,081 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.
2022-08-30T06:57:41,081 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661842661081
2022-08-30T06:57:41,081 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661842661081
2022-08-30T06:57:41,092 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - model_name: image_classifier, batchSize: 1
2022-08-30T06:57:41,093 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Backend worker process died.
2022-08-30T06:57:41,093 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-08-30T06:57:41,093 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_loader.py", line 100, in load
2022-08-30T06:57:41,093 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-08-30T06:57:41,093 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-08-30T06:57:41,093 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-08-30T06:57:41,093 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-08-30T06:57:41,094 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-08-30T06:57:41,094 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-08-30T06:57:41,094 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-08-30T06:57:41,094 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-08-30T06:57:41,094 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-08-30T06:57:41,094 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-08-30T06:57:41,095 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: image_classifier, error: Worker died.
2022-08-30T06:57:41,095 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: image_classifier, error: Worker died.
2022-08-30T06:57:41,095 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-08-30T06:57:41,095 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-08-30T06:57:41,095 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:41,095 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:41,095 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:41,095 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:41,095 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-08-30T06:57:41,095 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-08-30T06:57:41,095 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-08-30T06:57:41,096 [INFO ] W-9000-image_classifier_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:41,096 [INFO ] W-9000-image_classifier_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:41,103 [INFO ] W-9000-image_classifier_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:41,103 [INFO ] W-9000-image_classifier_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:44,096 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000]
2022-08-30T06:57:44,096 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000]
2022-08-30T06:57:44,632 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:57:44,633 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - [PID]160
2022-08-30T06:57:44,633 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Torch worker started.
2022-08-30T06:57:44,633 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-08-30T06:57:44,633 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Python runtime: 3.8.0
2022-08-30T06:57:44,633 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-08-30T06:57:44,633 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:57:44,633 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:57:44,634 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661842664634
2022-08-30T06:57:44,634 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.
2022-08-30T06:57:44,634 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661842664634
2022-08-30T06:57:44,643 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - model_name: image_classifier, batchSize: 1
2022-08-30T06:57:44,644 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Backend worker process died.
2022-08-30T06:57:44,644 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-08-30T06:57:44,645 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_loader.py", line 100, in load
2022-08-30T06:57:44,645 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-08-30T06:57:44,645 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-08-30T06:57:44,645 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-08-30T06:57:44,645 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-08-30T06:57:44,645 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-08-30T06:57:44,645 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-08-30T06:57:44,645 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-08-30T06:57:44,645 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-08-30T06:57:44,646 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-08-30T06:57:44,646 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-08-30T06:57:44,646 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-08-30T06:57:44,646 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-08-30T06:57:44,646 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'image_classifier'
2022-08-30T06:57:44,646 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - 
2022-08-30T06:57:44,646 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-08-30T06:57:44,646 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-08-30T06:57:44,646 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-08-30T06:57:44,646 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - 
2022-08-30T06:57:44,646 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-08-30T06:57:44,646 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: image_classifier, error: Worker died.
2022-08-30T06:57:44,646 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: image_classifier, error: Worker died.
2022-08-30T06:57:44,646 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-08-30T06:57:44,646 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-08-30T06:57:44,646 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-08-30T06:57:44,646 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     worker.run_server()
2022-08-30T06:57:44,647 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-08-30T06:57:44,647 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:44,647 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:44,647 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-08-30T06:57:44,647 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:44,647 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:44,647 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-08-30T06:57:44,647 [INFO ] W-9000-image_classifier_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:44,647 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-08-30T06:57:44,647 [INFO ] W-9000-image_classifier_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:44,647 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-08-30T06:57:44,655 [INFO ] W-9000-image_classifier_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:44,655 [INFO ] W-9000-image_classifier_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:46,219 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /172.30.0.1:59702 "GET /models/image_classifier HTTP/1.1" 200 18
2022-08-30T06:57:46,220 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842657
2022-08-30T06:57:49,648 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000]
2022-08-30T06:57:49,648 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000]
2022-08-30T06:57:50,173 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:57:50,173 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - [PID]184
2022-08-30T06:57:50,173 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Torch worker started.
2022-08-30T06:57:50,174 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Python runtime: 3.8.0
2022-08-30T06:57:50,174 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-08-30T06:57:50,174 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-08-30T06:57:50,174 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:57:50,174 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:57:50,175 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.
2022-08-30T06:57:50,175 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661842670175
2022-08-30T06:57:50,175 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661842670175
2022-08-30T06:57:50,183 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - model_name: image_classifier, batchSize: 1
2022-08-30T06:57:50,184 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Backend worker process died.
2022-08-30T06:57:50,184 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-08-30T06:57:50,184 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_loader.py", line 100, in load
2022-08-30T06:57:50,184 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-08-30T06:57:50,184 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-08-30T06:57:50,184 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-08-30T06:57:50,184 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-08-30T06:57:50,184 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-08-30T06:57:50,184 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-08-30T06:57:50,184 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-08-30T06:57:50,184 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-08-30T06:57:50,184 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-08-30T06:57:50,184 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-08-30T06:57:50,184 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-08-30T06:57:50,185 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-08-30T06:57:50,185 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'image_classifier'
2022-08-30T06:57:50,185 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-08-30T06:57:50,185 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - 
2022-08-30T06:57:50,185 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-08-30T06:57:50,185 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-08-30T06:57:50,185 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: image_classifier, error: Worker died.
2022-08-30T06:57:50,185 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - 
2022-08-30T06:57:50,185 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: image_classifier, error: Worker died.
2022-08-30T06:57:50,185 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-08-30T06:57:50,185 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-08-30T06:57:50,185 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-08-30T06:57:50,185 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-08-30T06:57:50,185 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:50,185 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     worker.run_server()
2022-08-30T06:57:50,185 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:50,186 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:50,186 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-08-30T06:57:50,186 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:50,186 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-08-30T06:57:50,186 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-08-30T06:57:50,186 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-08-30T06:57:50,186 [INFO ] W-9000-image_classifier_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:50,186 [INFO ] W-9000-image_classifier_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:50,196 [INFO ] W-9000-image_classifier_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:50,196 [INFO ] W-9000-image_classifier_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:58,187 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000]
2022-08-30T06:57:58,187 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000]
2022-08-30T06:57:58,710 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:57:58,711 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - [PID]204
2022-08-30T06:57:58,711 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Torch worker started.
2022-08-30T06:57:58,711 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Python runtime: 3.8.0
2022-08-30T06:57:58,711 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-08-30T06:57:58,711 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-08-30T06:57:58,711 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:57:58,711 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:57:58,712 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661842678712
2022-08-30T06:57:58,712 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661842678712
2022-08-30T06:57:58,712 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.
2022-08-30T06:57:58,722 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - model_name: image_classifier, batchSize: 1
2022-08-30T06:57:58,723 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Backend worker process died.
2022-08-30T06:57:58,723 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-08-30T06:57:58,723 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_loader.py", line 100, in load
2022-08-30T06:57:58,724 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-08-30T06:57:58,724 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_loader.py", line 162, in _load_handler_file
2022-08-30T06:57:58,724 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-08-30T06:57:58,724 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-08-30T06:57:58,724 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-08-30T06:57:58,724 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-08-30T06:57:58,724 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-08-30T06:57:58,724 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-08-30T06:57:58,724 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-08-30T06:57:58,724 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-08-30T06:57:58,724 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-08-30T06:57:58,724 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-08-30T06:57:58,724 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'image_classifier'
2022-08-30T06:57:58,724 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - 
2022-08-30T06:57:58,724 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-08-30T06:57:58,724 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-08-30T06:57:58,724 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-08-30T06:57:58,725 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: image_classifier, error: Worker died.
2022-08-30T06:57:58,725 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: image_classifier, error: Worker died.
2022-08-30T06:57:58,725 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - 
2022-08-30T06:57:58,725 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-08-30T06:57:58,725 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-08-30T06:57:58,725 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-08-30T06:57:58,725 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-08-30T06:57:58,725 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:58,725 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:58,725 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:58,725 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -     worker.run_server()
2022-08-30T06:57:58,725 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:58,725 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-08-30T06:57:58,725 [INFO ] W-9000-image_classifier_1.0-stdout MODEL_LOG -   File "/home/venv/lib/python3.8/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-08-30T06:57:58,725 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-08-30T06:57:58,725 [INFO ] W-9000-image_classifier_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:58,725 [INFO ] W-9000-image_classifier_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stdout
2022-08-30T06:57:58,736 [INFO ] W-9000-image_classifier_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stderr
2022-08-30T06:57:58,736 [INFO ] W-9000-image_classifier_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-image_classifier_1.0-stderr
2022-08-30T06:58:11,727 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000]
2022-08-30T06:58:11,727 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000]
2022-08-30T06:58:11,730 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stderr
2022-08-30T06:58:11,730 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stderr
2022-08-30T06:58:11,730 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stdout
2022-08-30T06:58:11,730 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stdout
2022-08-30T06:58:11,730 [ERROR] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Failed start worker process
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:145) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: java.io.IOException: Cannot run program "/home/venv/bin/python" (in directory "/tmp/models/image_classifier"): error=2, No such file or directory
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1143) ~[?:?]
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1073) ~[?:?]
	at java.lang.Runtime.exec(Runtime.java:594) ~[?:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:127) ~[model-server.jar:?]
	... 5 more
Caused by: java.io.IOException: error=2, No such file or directory
	at java.lang.ProcessImpl.forkAndExec(Native Method) ~[?:?]
	at java.lang.ProcessImpl.<init>(ProcessImpl.java:314) ~[?:?]
	at java.lang.ProcessImpl.start(ProcessImpl.java:244) ~[?:?]
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1110) ~[?:?]
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1073) ~[?:?]
	at java.lang.Runtime.exec(Runtime.java:594) ~[?:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:127) ~[model-server.jar:?]
	... 5 more
2022-08-30T06:58:11,730 [ERROR] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Failed start worker process
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:145) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: java.io.IOException: Cannot run program "/home/venv/bin/python" (in directory "/tmp/models/image_classifier"): error=2, No such file or directory
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1143) ~[?:?]
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1073) ~[?:?]
	at java.lang.Runtime.exec(Runtime.java:594) ~[?:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:127) ~[model-server.jar:?]
	... 5 more
Caused by: java.io.IOException: error=2, No such file or directory
	at java.lang.ProcessImpl.forkAndExec(Native Method) ~[?:?]
	at java.lang.ProcessImpl.<init>(ProcessImpl.java:314) ~[?:?]
	at java.lang.ProcessImpl.start(ProcessImpl.java:244) ~[?:?]
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1110) ~[?:?]
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1073) ~[?:?]
	at java.lang.Runtime.exec(Runtime.java:594) ~[?:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:127) ~[model-server.jar:?]
	... 5 more
2022-08-30T06:58:11,732 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2022-08-30T06:58:11,732 [DEBUG] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-image_classifier_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2022-08-30T06:58:11,732 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stderr
2022-08-30T06:58:11,732 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stderr
2022-08-30T06:58:11,732 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stdout
2022-08-30T06:58:11,732 [WARN ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-image_classifier_1.0-stdout
2022-08-30T06:58:11,732 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-08-30T06:58:11,732 [INFO ] W-9000-image_classifier_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-08-30T06:58:15,355 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-08-30T06:58:15,355 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-08-30T06:58:15,496 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/venv/lib/python3.8/site-packages
Current directory: /home/model-server
Temp directory: /home/model-server/tmp
Number of GPUs: 1
Number of CPUs: 16
Max heap size: 7992 M
Python executable: /home/venv/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /tmp/models
Initial Models: all
Log dir: /home/model-server/logs
Metrics dir: /home/model-server/logs
Netty threads: 32
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /tmp/models
Model config: N/A
2022-08-30T06:58:15,496 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/venv/lib/python3.8/site-packages
Current directory: /home/model-server
Temp directory: /home/model-server/tmp
Number of GPUs: 1
Number of CPUs: 16
Max heap size: 7992 M
Python executable: /home/venv/bin/python
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /tmp/models
Initial Models: all
Log dir: /home/model-server/logs
Metrics dir: /home/model-server/logs
Netty threads: 32
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /tmp/models
Model config: N/A
2022-08-30T06:58:15,504 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-08-30T06:58:15,504 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-08-30T06:58:15,520 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: densenet161.mar
2022-08-30T06:58:15,520 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: densenet161.mar
2022-08-30T06:58:16,547 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model densenet161
2022-08-30T06:58:16,547 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model densenet161
2022-08-30T06:58:16,547 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model densenet161
2022-08-30T06:58:16,547 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model densenet161
2022-08-30T06:58:16,547 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model densenet161 loaded.
2022-08-30T06:58:16,547 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model densenet161 loaded.
2022-08-30T06:58:16,547 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: densenet161, count: 1
2022-08-30T06:58:16,547 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: densenet161, count: 1
2022-08-30T06:58:16,553 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000]
2022-08-30T06:58:16,553 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000]
2022-08-30T06:58:16,555 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-08-30T06:58:16,555 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-08-30T06:58:16,603 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-08-30T06:58:16,603 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-08-30T06:58:16,604 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-08-30T06:58:16,604 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-08-30T06:58:16,604 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-08-30T06:58:16,604 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-08-30T06:58:16,605 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-08-30T06:58:16,605 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-08-30T06:58:16,605 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-08-30T06:58:16,605 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-08-30T06:58:16,777 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-08-30T06:58:16,777 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-08-30T06:58:17,147 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:58:17,147 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - [PID]55
2022-08-30T06:58:17,148 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Torch worker started.
2022-08-30T06:58:17,148 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Python runtime: 3.8.0
2022-08-30T06:58:17,148 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change null -> WORKER_STARTED
2022-08-30T06:58:17,148 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change null -> WORKER_STARTED
2022-08-30T06:58:17,152 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:58:17,152 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2022-08-30T06:58:17,160 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.
2022-08-30T06:58:17,163 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661842697163
2022-08-30T06:58:17,163 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661842697163
2022-08-30T06:58:17,188 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - model_name: densenet161, batchSize: 1
2022-08-30T06:58:17,341 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842697
2022-08-30T06:58:17,341 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:49.36098861694336|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842697
2022-08-30T06:58:17,342 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:394.69104385375977|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842697
2022-08-30T06:58:17,342 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:88.9|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842697
2022-08-30T06:58:17,342 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:10.953776041666666|#Level:Host,device_id:0|#hostname:148f9db9e5dd,timestamp:1661842697
2022-08-30T06:58:17,342 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:673|#Level:Host,device_id:0|#hostname:148f9db9e5dd,timestamp:1661842697
2022-08-30T06:58:17,343 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:64|#Level:Host,device_id:0|#hostname:148f9db9e5dd,timestamp:1661842697
2022-08-30T06:58:17,343 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:24538.32421875|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842697
2022-08-30T06:58:17,343 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6816.9140625|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842697
2022-08-30T06:58:17,343 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:23.2|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842697
2022-08-30T06:58:19,469 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2281
2022-08-30T06:58:19,469 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2281
2022-08-30T06:58:19,470 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-08-30T06:58:19,470 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-08-30T06:58:19,470 [INFO ] W-9000-densenet161_1.0 TS_METRICS - W-9000-densenet161_1.0.ms:2919|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842699
2022-08-30T06:58:19,470 [INFO ] W-9000-densenet161_1.0 TS_METRICS - WorkerThreadTime.ms:26|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842699
2022-08-30T06:58:34,642 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661842714642
2022-08-30T06:58:34,642 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661842714642
2022-08-30T06:58:34,644 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Backend received inference at: 1661842714
2022-08-30T06:58:37,027 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2384
2022-08-30T06:58:37,027 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2384
2022-08-30T06:58:37,028 [INFO ] W-9000-densenet161_1.0 ACCESS_LOG - /172.30.0.1:51638 "PUT /predictions/densenet161 HTTP/1.1" 200 2390
2022-08-30T06:58:37,029 [INFO ] W-9000-densenet161_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842714
2022-08-30T06:58:37,029 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.job.Job - Waiting time ns: 151526, Backend time ns: 2386388341
2022-08-30T06:58:37,029 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.job.Job - Waiting time ns: 151526, Backend time ns: 2386388341
2022-08-30T06:58:37,029 [INFO ] W-9000-densenet161_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842717
2022-08-30T06:58:37,029 [INFO ] W-9000-densenet161_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:148f9db9e5dd,timestamp:1661842717
2022-08-30T06:58:37,027 [INFO ] W-9000-densenet161_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:2382.65|#ModelName:densenet161,Level:Model|#hostname:148f9db9e5dd,requestID:6a0b484c-b18f-4778-8ce3-c57cce484195,timestamp:1661842717
2022-08-30T06:58:37,030 [INFO ] W-9000-densenet161_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:2382.71|#ModelName:densenet161,Level:Model|#hostname:148f9db9e5dd,requestID:6a0b484c-b18f-4778-8ce3-c57cce484195,timestamp:1661842717
